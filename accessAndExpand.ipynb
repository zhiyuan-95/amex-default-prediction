{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyuan-95/amex-default-prediction/blob/master/accessAndExpand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHS-qRjXWR64"
      },
      "source": [
        "# get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxT-YGI84SG6",
        "outputId": "f822234b-d6cd-44e9-fa25-03e30f902382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZIKN81g5oHa"
      },
      "source": [
        "### download data from kaggle ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "071YeUgUk5eE",
        "outputId": "07aa3472-c88d-454c-d240-5be92b66ec2f"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5a740ace4027>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mapi_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'kaggle competitions download -c amex-default-prediction'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mconfig_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_config_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[1;32m    165\u001b[0m                               ' {}. Or use the environment method.'.format(\n\u001b[1;32m    166\u001b[0m                                   self.config_file, self.config_dir))\n",
            "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."
          ]
        }
      ],
      "source": [
        "import kaggle\n",
        "import subprocess\n",
        "api_command = 'kaggle competitions download -c amex-default-prediction'\n",
        "subprocess.run(api_command, shell = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5-T2V-p56uW"
      },
      "source": [
        "### unzip to colab##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwU0Ku_c76sD",
        "outputId": "0c2c9b38-543c-4e20-b4b6-f9c9c03aee81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avhl1YJt2o5T"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('amex-default-prediction.zip','r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IJGGI4_6KcR"
      },
      "source": [
        "### upload to google storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu3dS_qkwN7I"
      },
      "outputs": [],
      "source": [
        "import google.colab\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDB3gMMwUjf"
      },
      "outputs": [],
      "source": [
        "google.colab.auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBqgQuoUwEMi"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "bucket_name = 'aedp'\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "file_path = '/content/train_labels.csv'\n",
        "blob = bucket.blob('train_labels.csv')\n",
        "blob.upload_from_filename(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5_2ratz4hN"
      },
      "source": [
        "# data expansion\n",
        "###1. get numerical part of data and categorical part of data\n",
        "\n",
        "###2. expand the data set\n",
        "\n",
        "*   for the numerical part, using the describe() function in pandas to get the statistic of a customer's bank statement variables\n",
        "*   for the categorical part('D_63', 'D_64', 'B_31'), also use describe in pandas to get statistic\n",
        "\n",
        "###3. combine those two part of information, and also combine all the customers in rdd then we get a new data set\n",
        "\n",
        "###4. those processes are run on the dataproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOnhhULoJ8aT",
        "outputId": "2fd913cb-a08d-4b43-fb6b-7d96ff5915bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=JVLt8Hl0cb0RXybNkhKhjJkazvbWZC&prompt=consent&access_type=offline&code_challenge=1ZsOOJHw1tAX74W6KjjqItLOJhx6fI1eLUvyyHGAKFM&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhXDpc6EHeJ-5OnWALfNPgFxzlo3adAexVAeBTNjR_KmnDQq6EyCChufZUbsmbwKcg\n",
            "\n",
            "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEyBhY_0ujT",
        "outputId": "02cfcd5e-ccdf-4a53-80b5-cc651408d97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://aedp/original_data/train_data.csv...\n",
            "/ [0 files][    0.0 B/ 15.3 GiB]                                                \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "Resuming download for /content/training_set.csv\n",
            "/ [1 files][ 15.3 GiB/ 15.3 GiB]  101.0 MiB/s                                   \n",
            "Operation completed over 1 objects/15.3 GiB.                                     \n",
            "Copying gs://aedp/original_data/train_labels.csv...\n",
            "/ [1 files][ 29.3 MiB/ 29.3 MiB]                                                \n",
            "Operation completed over 1 objects/29.3 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://aedp/original_data/train_data.csv /content/training_set.csv\n",
        "!gsutil cp gs://aedp/original_data/train_labels.csv /content/label.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y9kI7qkTMOK",
        "outputId": "c22baa6a-4337-4f1d-fdc9-0cfb36013a58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMMH1HYARlAA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RdcBs694oGr"
      },
      "outputs": [],
      "source": [
        "rdd_feature = sc.textFile('training_set.csv')\n",
        "rdd_labels = sc.textFile('label.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5e3x9XLUsZ4"
      },
      "outputs": [],
      "source": [
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUY-d_vfW3m5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N3vKyx7li_l"
      },
      "outputs": [],
      "source": [
        "## get a smaller sample to try on local device\n",
        "sample_rdd = sc.parallelize(rdd_feature.take(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy86HHaH4wXq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "sample = sample_rdd.mapPartitionsWithIndex(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgVGbeyUuhst"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI1LKhvMo_8X"
      },
      "outputs": [],
      "source": [
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh2_jzJK95Xf"
      },
      "outputs": [],
      "source": [
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE-J0UpsUtW0"
      },
      "outputs": [],
      "source": [
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQv9XA_1gBBV"
      },
      "outputs": [],
      "source": [
        "expansion = sample.mapPartitionsWithIndex(encode_cat)\n",
        "    map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR0lXzG1fkHs"
      },
      "outputs": [],
      "source": [
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9NnBP3Mnza-"
      },
      "outputs": [],
      "source": [
        "num_col = []\n",
        "cat_col = []\n",
        "\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = '.'.join(num_col+cat_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dsYKk4HN1ik"
      },
      "outputs": [],
      "source": [
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(add_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv9nIiiTPC8G",
        "outputId": "441b67d3-98a0-4a5b-f8dc-1bca14ac78ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.create) You do not currently have an active account selected.\n",
            "Please run:\n",
            "\n",
            "  $ gcloud auth login\n",
            "\n",
            "to obtain new credentials.\n",
            "\n",
            "If you have already logged in with a different account, run:\n",
            "\n",
            "  $ gcloud config set account ACCOUNT\n",
            "\n",
            "to select an already authenticated account to use.\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters create expansion --enable-component-gateway --region us-central1 --zone us-central1-b --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bda-12345"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31g8J5WgYej6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6df0d3-1932-4c7c-8a9a-93e66adc49b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project bda-12345\n",
        "!gcloud config set compute/region us-central1\n",
        "!gcloud config set compute/zone us-central1-a\n",
        "!gcloud config set dataproc/region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkASK62bGFTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def725a-8718-4208-f5c1-515268322cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting expansion.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/train_data.csv')\n",
        "rdd_labels = sc.textFile('gs://aedp/original_data/train_labels.csv')\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\n",
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(add_labels).saveAsTextFile('gs://aedp/expanded_data/expanded_data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/test_data.csv')\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(expansion).saveAsTextFile('gs://aedp/expanded_data/expanded_test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v223iKne2PfH",
        "outputId": "1c447d59-1489-43bc-b489-9374de10f723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_expansion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IenUn36Yb56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c93924c-ae22-4a60-f71a-ba327fd06861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [4252a113eede4f1c8302fbb691de7606] submitted.\n",
            "Waiting for job output...\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.util.log: Logging initialized @3727ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_372-b07\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.Server: Started @3801ms\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@729c8284{HTTP/1.1, (http/1.1)}{0.0.0.0:34503}\n",
            "23/07/17 22:09:56 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at expansion-m/10.128.0.38:8032\n",
            "23/07/17 22:09:56 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at expansion-m/10.128.0.38:10200\n",
            "23/07/17 22:09:57 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/07/17 22:09:57 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/17 22:09:58 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1689534591183_0001\n",
            "23/07/17 22:09:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at expansion-m/10.128.0.38:8030\n",
            "23/07/17 22:10:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=154; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-369021837527-83jn524y/650cf5e8-c690-41b8-b42b-6f7b268a58fe/spark-job-history\n",
            "23/07/17 22:10:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/17 22:10:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=176; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-369021837527-83jn524y/650cf5e8-c690-41b8-b42b-6f7b268a58fe/spark-job-history\n",
            "23/07/17 22:10:02 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "23/07/17 22:10:13 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=235; previousMaxLatencyMs=176; operationCount=2; context=gs://aedp/expanded_data/expanded_test/_temporary/0\n",
            "23/07/18 16:33:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/expanded_test/' directory.\n",
            "23/07/18 16:33:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=363; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/expanded_test/_temporary\n",
            "23/07/18 16:33:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=201; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/expanded_test/_SUCCESS\n",
            "23/07/18 16:33:01 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@729c8284{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [4252a113eede4f1c8302fbb691de7606] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/driveroutput\n",
            "jobUuid: 2fe5bee0-50f9-31e4-94da-0f432c40a434\n",
            "placement:\n",
            "  clusterName: expansion\n",
            "  clusterUuid: 650cf5e8-c690-41b8-b42b-6f7b268a58fe\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/staging/test_expansion.py\n",
            "reference:\n",
            "  jobId: 4252a113eede4f1c8302fbb691de7606\n",
            "  projectId: bda-12345\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-07-18T16:33:05.948191Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-07-17T22:09:49.511013Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-07-17T22:09:49.559395Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-07-17T22:09:49.883040Z'\n",
            "yarnApplications:\n",
            "- name: test_expansion.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://expansion-m:8088/proxy/application_1689534591183_0001/\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster expansion test_expansion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ObWYKY9OOvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b9b0f3-dc3b-460c-a601-1f7bbbbdc554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.delete) Error parsing [cluster].\n",
            "The [cluster] resource is not properly specified.\n",
            "Failed to find attribute [project]. The attribute can be set in the following ways: \n",
            "- provide the argument `--project` on the command line\n",
            "- set the property `core/project`\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters delete expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbIRc14It7nm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxAlo0hSOYZgrBxzooYFqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}